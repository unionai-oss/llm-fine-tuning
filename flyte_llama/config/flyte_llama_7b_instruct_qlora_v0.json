{
    "model_path": "codellama/CodeLlama-7b-Instruct-hf",
    "output_dir": "./output",
    "checkpoint_dir": null,
    "num_epochs": 1,
    "batch_size": 1,
    "test_size": 0.001,
    "model_max_length": 4096,
    "seed": 41,
    "report_to": "wandb",
    "device_map": "auto",
    "gradient_accumulation_steps": 8,
    "padding": "left",
    "dataloader_num_proc": 8,
    "use_fp16": true,
    "use_4bit": true,
    "use_qlora": true,
    "lora_r": 8,
    "lora_alpha": 16,
    "lora_target_modules": ["q_proj", "v_proj"],
    "lora_dropout": 0.05,
    "debug": false,
    "publish_config": {
        "repo_id": "unionai/FlyteLlama-v0-7b-Instruct-hf",
        "readme": "# FlyteLlama-v0-7b-Instruct-hf fine-tuned on Flyte repos",
        "language": "python",
        "model_card": {
            "language": ["en"],
            "license": "apache-2.0",
            "tags": [
                "pytorch",
                "causal-lm",
                "llama2",
                "code llama",
                "fine-tuning",
                "flyte llama",
                "flyte repo dataset"
            ]
        }
    }
}
